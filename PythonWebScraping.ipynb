{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Web Scraping in Python\n",
    "\n",
    "Today we will learn how to scrape HTML web pages in python, using the Beautiful Soup 4 library. We can programmatically gather information from websites to use for your own purposes. We will gather information about prominent UVA employees, first by walking through each step of the process. Then by doing it in a more automated way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to install the Beautiful Soup 4 library. This is not a part of base python or the Anaconda installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install --yes beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's import the libraries we will be using in this Jupyter Notebook, including Beautiful Soup 4. The other two, requests and pandas, are already installed if you are using Anaconda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to make an HTTP request using the requests library. This means that once you have established a connection with the destination (the server which hosts the website you want to communicate with), the client (you) sends an HTTP GET request to the server to retrieve the website and all data within it. \n",
    "\n",
    "This is typically done by your web browser, but we can also do it in python. \n",
    "\n",
    "Notice that we are looking at the publicly available employee information for Tony Bennett, UVA's men's basketball coach. All Virginia public employees' salary (and other) information is publicly available thanks to the Richmond Times-Dispatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is how to make an HTTP GET request using the requests library.\n",
    "source = requests.get(f'https://data.richmond.com/salaries/2018/state/university-of-virginia/tony-bennett')\n",
    "\n",
    "print(source)      #this prints the type of response. 200 means \"OK\". There are many response codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a response from the server. If the response is good, the source code of the web page is contained within that response. Let's see what that looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(source.text, 'html.parser')\n",
    "\n",
    "print(soup) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is messy, but it is all the code for the page we have issued a request for. Some of it is human readable, some of it is not. Now, let's look at the source code of this page another way. Copy and paste this link into your web browser: https://data.richmond.com/salaries/2018/state/university-of-virginia/tony-bennett\n",
    "\n",
    "Right click somewhere on your page and \"view page source\", then right click again and \"inspect elements\". While inspecting the page elements, you can see which parts of the page are controlled by different parts of the code. Notice that the code starts with large chunks (< body > for example), and has divisions within that (< div > tags), among others.\n",
    "\n",
    "The class \"container\" looks like it contains the majority of the information in the body of the page. Let's start with this. We are going to scrape some information about Tony Bennett from this page. Specifically, his Salary and Job Title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the prettify() function makes the code somewhat more readable.\n",
    "# I don't use this feature much but maybe you will appreciate it.\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The find() function finds the first item matching this criteria. Notice our arguments are first, the HTML tag, and second, the class within that tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = soup.find(\"div\", class_='container')    #class_, because 'class' is a reserved word in python\n",
    "print(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, I know there are actually 3 container classes in this web site. So let's use the find_all() function to get the information from all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = soup.find_all('div', class_='container')\n",
    "print(container)\n",
    "print()\n",
    "print(len(container))  #container is just a list! See there are 3 items in this list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2nd item in the container object is the one that contains the information we want, so let's rename that.\n",
    "\n",
    "Then, the next < div > class under that is 'row col-12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = container[1]\n",
    "\n",
    "row_12 = container.find_all('div', class_='row col-12')\n",
    "\n",
    "print(row_12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as you see, you can step through each tag on your way to the information you need. In order to speed up this process I will take some shortcuts to get to Tony Bennett's job title and salary.\n",
    "\n",
    "You don't have to step through each HTML tag to get to what you want. You can identify the tag you need and go straight to it.\n",
    "\n",
    "Below you see how I go straight from the \"container\" class to the individual element holding hist job title. For his salary I find an 'h2' class which contains this information. Also, note that I am using the find() function because there is only one instance of each of these specific classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title = container.find('span', class_='small text-muted')\n",
    "\n",
    "salary = container.find('h2', class_='pay')\n",
    "\n",
    "print(job_title.text)\n",
    "print(salary.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do a more programmatic example. Every UVA employee has a page with basically the same URL, except the person's name is different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's do a little more interesting example\n",
    "names = ['Tony Bennett', 'James E Ryan', 'Bronco Mendenhall', 'Carla Williams', 'Scott C Beardsley', 'Craig Benson', \n",
    "        'Ian Baucom']\n",
    "\n",
    "formatted_names_of_important_people = []\n",
    "\n",
    "#start with a little string formatting. I am formatting each name so I can insert it into the URL\n",
    "for important_person in names:\n",
    "    important_person = important_person.replace(' ', '-')\n",
    "    important_person = important_person.lower()\n",
    "    formatted_names_of_important_people.append(important_person)\n",
    "\n",
    "print(formatted_names_of_important_people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now I will use f-strings formatting to insert each name into the source URL\n",
    "for name in formatted_names_of_important_people:\n",
    "    \n",
    "    source = requests.get(f'https://data.richmond.com/salaries/2018/state/university-of-virginia/{name}')\n",
    "    soup = BeautifulSoup(source.text, 'html.parser')\n",
    "\n",
    "    main_box = soup.find(\"div\", class_='pay')\n",
    "    salary = main_box.find('h2').text\n",
    "    \n",
    "    print(name, salary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#but let's make this look a little nicer\n",
    "\n",
    "for name in formatted_names_of_important_people:\n",
    "    \n",
    "    source = requests.get(f'https://data.richmond.com/salaries/2018/state/university-of-virginia/{name}') \n",
    "    soup = BeautifulSoup(source.text, 'html.parser')\n",
    "    \n",
    "    main_box = soup.find(\"div\", class_='pay')\n",
    "    salary = main_box.find('h2').text\n",
    "\n",
    "    main_box = soup.find(\"div\", class_='col-12 col-lg-8')\n",
    "    span_class = main_box.find_all(\"span\")\n",
    "    job_title = span_class[1].text\n",
    "        \n",
    "    print(f'Name = {name}')\n",
    "    print(f'Job Title = {job_title}')\n",
    "    print(f'Salary = {salary}')     \n",
    "    print()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A common thing you might do is take this data and use it for your own purposes elsewhere.\n",
    "#Let's take this data we have scraped and put it into a pandas DataFrame\n",
    "\n",
    "data = []\n",
    "\n",
    "for name in formatted_names_of_important_people:\n",
    "    \n",
    "    source = requests.get(f'https://data.richmond.com/salaries/2018/state/university-of-virginia/{name}')\n",
    "    soup = BeautifulSoup(source.text, 'html.parser')\n",
    "\n",
    "    main_box = soup.find(\"div\", class_='pay')\n",
    "    salary = main_box.find('h2').text\n",
    "    \n",
    "    #data.append((name, salary))\n",
    "    data.append((name, salary))\n",
    "    \n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put this into a pandas Dataframe!\n",
    "\n",
    "## Pandas ##\n",
    "\n",
    "Pandas is an open source python library providing high-performance, easy to use data structures. It is common to store data scraped from the web in a pandas Dataframe.\n",
    "\n",
    "A pandas Dataframe is a 2 Dimensional data structure with rows and columns (like a spreadsheet). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's rename the columns in the pandas dataframe to something more descriptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.columns = ['Name', 'Salary']\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
