{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Web Scraping in Python\n",
    "\n",
    "Today we will learn how to scrape HTML web pages in python, using the Beautiful Soup 4 library. We can programmatically gather information from websites to use for your own purposes. We will gather information about the UVA basketball team, first by walking through each step of the process. Then by doing it in a more automated way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to install the Beautiful Soup 4 and lxml libraries. These are not a part of base python or the Anaconda installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install --yes beautifulsoup4\n",
    "!conda install --yes lxml\n",
    "!conda install --yes requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's import the libraries we will be using in this Jupyter Notebook, including Beautiful Soup 4. The other two, requests and pandas, are already installed if you are using Anaconda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import lxml\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to make an HTTP request using the requests library. This means that once you have established a connection with the destination (the server which hosts the website you want to communicate with), the client (you) sends an HTTP GET request to the server to retrieve the website and all data within it. \n",
    "\n",
    "This is typically done by your web browser, but we can also do it in python. \n",
    "\n",
    "Today we are going to scrape information about players on the UVA men's basketball team. You can find the team's roster for the 2024-2025 season here: https://virginiasports.com/sports/mbball/roster/ \n",
    "\n",
    "We will start by gathering information about one of UVA's players, Blake Buchanan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is how to make an HTTP GET request using the requests library.\n",
    "source = requests.get('https://virginiasports.com/sports/mbball/roster/season/2024-25/player/blake-buchanan/')\n",
    "\n",
    "print(source)      #this prints the type of response. 200 means \"OK\". There are many response codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a response from the server. If the response is good, the source code of the web page is contained within that response. Let's see what that looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(source.text, 'lxml')\n",
    "\n",
    "print(soup) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "This is messy, but it is all the code for the page we have issued a request for. Some of it is human readable, some of it is not. Now, let's look at the source code of this page another way. Copy and paste this link into your web browser: https://virginiasports.com/sports/mbball/roster/season/2024-25/player/blake-buchanan/\n",
    "\n",
    "### Note - You need to use Google Chrome to have access to inspector and other developer tools\n",
    "\n",
    "Right click somewhere on your page and click \"inspect\". Then, make sure to choose the 'elements' tab to see the HTML source code of this page. While inspecting the page elements, you can see which parts of the page are controlled by different parts of the code. Notice that the code starts with large chunks (< body > for example), and has divisions within that (< div > tags), among others.\n",
    "\n",
    "The class \"bio-info\" looks like it contains the majority of the information in the body of the page. Let's start with this. We are going to scrape some information about Blake Buchanan from this page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the prettify() function makes the code somewhat more readable.\n",
    "# I don't use this feature much but maybe you will appreciate it.\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The find() function finds the first item matching this criteria. Notice our arguments are first, the HTML tag, and second, the class within that tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_info = soup.find(\"div\", class_='bio-info')    #class_, because 'class' is a reserved word in python\n",
    "print(player_info.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now see that the player's name is inside a couple more tags inside that 'bio-info' class. Let's drill down into the code and get the player's name value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_name = soup.find('div', class_='bio-info').div.h1.text\n",
    "print(player_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I want more information about the player such as his position, height, weight, etc. \n",
    "\n",
    "Looking again at the HTML, it looks like all of that information is in repetitive 'div' tags. Let's select all of those elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in soup.find_all('div', class_=\"value\"):\n",
    "    print(item.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also want to get the labels for this data. This would be things such as 'position', 'height', 'weight', etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in soup.find_all('div', class_='description'):\n",
    "    print(item.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do a little more here just to clean things up. In the following cell I will do the same as the previous two cells, but this time I will store the data in a list. Then I will zip those lists together and make a nice dictionary with my data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_stats=[]\n",
    "labels=[]\n",
    "\n",
    "for i in soup.find_all('div', class_='value'):\n",
    "    player_stats.append(i.text)\n",
    "    \n",
    "for i in soup.find_all('div', class_='description'):\n",
    "    labels.append(i.text)\n",
    "    \n",
    "#let's assume I only want the first five items in each list\n",
    "player_stats = player_stats[:5]\n",
    "labels = labels[:5]\n",
    "\n",
    "player_dict = dict(zip(labels, player_stats))\n",
    "print(player_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have an easy to use dictionary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(player_dict['Hometown'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium\n",
    "\n",
    "Now that we have learned to scrape static HTML content, let's automate this task using Selenium. Check out the Selenium documentation here: https://www.selenium.dev/\n",
    "\n",
    "### Selenium Web Driver\n",
    "In order to use Selenium, we must download and install a web driver which allows you to drive a browser with your code.\n",
    "\n",
    "**Important**\n",
    "The following code assumes you are using Google Chrome and will use the associated web driver. If you are using another browser (safari, firefox, edge, etc) you will need to download the selenium web driver for that browser. Just check the Selenium documentation in order to do that.\n",
    "\n",
    "You also need to make sure to download the correct web driver which corresponds to your version of Google Chrome.\n",
    "\n",
    "I have included a detailed writeup about this in the 'WebDriverInstall.md' file in the github repository.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install selenium\n",
    "!conda install --yes selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import selenium and webdrivers\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure now that Selenium is working and you have all your paths set up correctly. \n",
    "\n",
    "If this works correctly, it will open up a blank browser with a message that 'this is being controlled by automated test software'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that Selenium is working and you have all your 'MY_PATH' variable set up correctly\n",
    "# example: /Users/ep9k/Desktop/PythonWebScraping-master/chromedriver\n",
    "MY_PATH = \"./chromedriver\"\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=MY_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go directly to a page of our choice like so..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(executable_path=MY_PATH)\n",
    "driver.get('https://virginiasports.com/sports/mbball/roster/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now proceed to write out script just like any other program. Just like BeautifulSoup, Selenium provides the ability to select HTML elements by the tag name, class name, id name, and so on. \n",
    "\n",
    "On the UVA roster homepage, looking at the HTML you can see that each player has it's own box with a picture, name, and so on. The HTML is the same for all of those players. In our example, we will simulate that the user is clicking on each player to see that player's individual page. Then we will scrape the information off of that page just like we did in earler. Let's start with just one player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(executable_path=MY_PATH)\n",
    "driver.get('https://virginiasports.com/sports/mbball/roster/')\n",
    "\n",
    "#I am selecting by x path. xpath can be used to navigate through XML documents\n",
    "player = driver.find_element_by_xpath('//*[@id=\"players\"]/div[1]/div[2]/div[1]')\n",
    "player.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "See above that I used the 'find_element_by_xpath' method to click on this player's name. Let's define what an XPath is.\n",
    "\n",
    "**XPath**: XPath enables testers to navigate through the XML structure of an HTML or XML document. Don't worry too much about this. I like to use 'find_element_by_xpath' because it is basically a unique identifier for items in the HTML document and makes selecting them easy.\n",
    "\n",
    "To select an element (by XPath or another way), right click on the thing on the page you are interested in, click \"inspect\", then in the console in the \"elements\" tab right click on the HTML of that thing, click \"copy\", click \"XPath\".\n",
    "\n",
    "There are many different ways to select HTML elements in Selenium. Check out the documentation for more examples: https://selenium-python.readthedocs.io/locating-elements.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have Selenium installed and set up and we did a small example, we can now expand upon it. We will take just a few players from the team and iterate through them. Once we land on each page we will do exactly what we just did with static HTML with BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(executable_path=MY_PATH)\n",
    "driver.get('https://virginiasports.com/sports/mbball/roster/')\n",
    "\n",
    "\n",
    "#we will use a while loop to grab data about the first three players\n",
    "count = 0\n",
    "\n",
    "while count < 3:\n",
    "    count += 1\n",
    "    time.sleep(1)\n",
    "    \n",
    "    #select player's name using find_element_by_xpath()\n",
    "    #each player's name is a link to their bio page\n",
    "    player = driver.find_element_by_xpath(f'//*[@id=\"players\"]/div[{count}]/div[2]/div[1]')\n",
    "    player.click()\n",
    "    \n",
    "    #now we use beautiful soup to parse the HTML just as we did last time\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    \n",
    "    player_name = soup.find('div', class_='bio-info').div.h1.text\n",
    "    print()\n",
    "    print(player_name)\n",
    "    \n",
    "    for item in soup.find_all('div', class_=\"value\"):\n",
    "        print(item.text) \n",
    "        \n",
    "    #go back to roster page after collecting information about current player    \n",
    "    driver.get('https://virginiasports.com/sports/mbball/roster/')\n",
    "    \n",
    "driver.quit()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do just a little more to make it pretty. This time we will collect the information about each player and put it into a pandas dataframe. There are many ways to do this but I will use lists to populate the columns of the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the whole thing together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "driver = webdriver.Chrome(executable_path=MY_PATH)\n",
    "driver.get('https://virginiasports.com/sports/mbball/roster/')\n",
    "\n",
    "#accumulator lists we will use later to make our pandas dataframe\n",
    "names = []\n",
    "positions = []\n",
    "heights = []\n",
    "weights = []\n",
    "years = []\n",
    "\n",
    "\n",
    "#we will use a while loop to grab data about the first three players\n",
    "count = 0\n",
    "\n",
    "team_players = ['Blake Buchanan', 'Jalen Warley', 'Elijah Saunders', 'Andrew Rohde', \n",
    "               'Jacob Cofie', 'Dai Dai Ames', 'Bryce Walker', 'Ishan Sharma', \n",
    "                'Anthony Robinson', 'Taine Murray', 'Isaac McKneely', 'Elijah Gertude',\n",
    "                'Desmond Roberts','TJ Power', 'Christian Bliss', 'Carter Lang' ]\n",
    "\n",
    "#there are 16 players on the team\n",
    "for i in team_players:\n",
    "    count += 1\n",
    "    time.sleep(1)\n",
    "    \n",
    "    #select player's name using find_element_by_xpath()\n",
    "    #each player's name is a link to their bio page\n",
    "    player = driver.find_element_by_xpath(f'//*[@id=\"players\"]/div[{count}]/div[2]/div[1]')\n",
    "    \n",
    "    player.click()\n",
    "    \n",
    "    #now we use beautiful soup to parse the HTML just as we did last time\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    \n",
    "    #player info will be used to store the data about each player in a list\n",
    "    #this will look like:  ['Blake Buchanan', 'Forward', '6'11\"', '235 lbs', 'Sophomore']\n",
    "    player_info = []\n",
    "    \n",
    "    for item in soup.find_all('div', class_=\"value\"):\n",
    "        player_info.append(item.text)\n",
    "        \n",
    "    #add the information from player_info to the accumulator lists outside this loop\n",
    "    names.append(i)    # i is player name from team_players\n",
    "    positions.append(player_info[0])\n",
    "    heights.append(player_info[1])\n",
    "    weights.append(player_info[2])\n",
    "    years.append(player_info[3])\n",
    "    \n",
    "    #go back to roster page after collecting information about current player    \n",
    "    driver.get('https://virginiasports.com/sports/mbball/roster/')\n",
    "    \n",
    "driver.quit()\n",
    "\n",
    "print(names)\n",
    "print(positions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we will take the lists we created above and put them into the pandas dataframe we created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make empty dataframe\n",
    "bball_team_df = pd.DataFrame()\n",
    "\n",
    "# Take lists of team information and put them into columns of dataframe\n",
    "bball_team_df['Name'] = names\n",
    "bball_team_df['Position'] = positions\n",
    "bball_team_df['Height'] = heights\n",
    "bball_team_df['Weight'] = weights\n",
    "bball_team_df['Year'] = years\n",
    "print(bball_team_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
