{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Web Scraping in Python\n",
    "\n",
    "Today we will learn how to scrape HTML web pages in python, using the Beautiful Soup 4 library. We can programmatically gather information from websites to use for your own purposes. We will gather information about the UVA basketball team, first by walking through each step of the process. Then by doing it in a more automated way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to install the Beautiful Soup 4 and lxml libraries. These are not a part of base python or the Anaconda installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: | \n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - defaults/osx-64::r-base64enc==0.1_3=r36h46e59ec_4\n",
      "  - defaults/noarch::r-dbplyr==1.4.0=r36h6115d3f_0\n",
      "  - defaults/osx-64::r-xml2==1.2.0=r36h466af19_0\n",
      "  - defaults/noarch::r-bh==1.69.0_1=r36h6115d3f_0\n",
      "  - defaults/osx-64::argon2-cffi==20.1.0=py37haf1e3a3_1\n",
      "  - defaults/osx-64::r-bit64==0.9_7=r36h46e59ec_0\n",
      "  - defaults/osx-64::r-askpass==1.0=r36h1de35cc_0\n",
      "  - defaults/osx-64::jupyter==1.0.0=py37_7\n",
      "  - defaults/noarch::r-xfun==0.6=r36h6115d3f_0\n",
      "  - defaults/osx-64::r-bitops==1.0_6=r36h46e59ec_4\n",
      "  - defaults/osx-64::r-yaml==2.2.0=r36h46e59ec_0\n",
      "  - defaults/osx-64::bcrypt==3.2.0=py37haf1e3a3_0\n",
      "  - defaults/osx-64::r-base==3.6.1=hcb44179_1\n",
      "  - defaults/osx-64::widgetsnbextension==3.5.1=py37_0\n",
      "  - defaults/noarch::r-generics==0.0.2=r36h6115d3f_0\n",
      "  - defaults/noarch::numpydoc==1.1.0=py_0\n",
      "  - defaults/osx-64::notebook==6.1.1=py37_0\n",
      "  - defaults/noarch::r-packrat==0.5.0=r36h6115d3f_0\n",
      "  - defaults/osx-64::selenium==3.141.0=py37h9ed2024_1000\n",
      "  - defaults/noarch::r-forge==0.2.0=r36h6115d3f_0\n",
      "  - defaults/osx-64::r-fansi==0.4.0=r36h46e59ec_0\n",
      "  - defaults/noarch::r-stringr==1.4.0=r36h6115d3f_0\n",
      "  - defaults/osx-64::r-openssl==1.3=r36h46e59ec_0\n",
      "  - defaults/noarch::r-shiny==1.3.2=r36h6115d3f_0\n",
      "  - defaults/osx-64::r-mongolite==2.0.1=r36h46e59ec_0\n",
      "  - defaults/noarch::r-rmarkdown==1.12=r36h6115d3f_0\n",
      "  - defaults/osx-64::r-catools==1.17.1.2=r36h466af19_0\n",
      "  - defaults/osx-64::r-haven==2.1.0=r36h466af19_0\n",
      "  - defaults/osx-64::clangxx_osx-64==10.0.0=h05bbb7f_1\n",
      "  - defaults/osx-64::r-profvis==0.3.5=r36h46e59ec_0\n",
      "  - defaults/noarch::ipywidgets==7.5.1=py_0\n",
      "  - defaults/osx-64::r-digest==0.6.18=r36h46e59ec_0\n",
      "  - defaults/osx-64::r-promises==1.0.1=r36h466af19_0\n",
      "  - defaults/osx-64::r-rjava==0.9_11=r36h46e59ec_0\n",
      "  - defaults/osx-64::r-stringi==1.4.3=r36h466af19_0\n",
      "  - defaults/noarch::r-tinytex==0.12=r36h6115d3f_0\n",
      "  - defaults/osx-64::r-tidyselect==0.2.5=r36h466af19_0\n",
      "  - defaults/noarch::urllib3==1.25.10=py_0\n",
      "  - defaults/noarch::anaconda-project==0.8.4=py_0\n",
      "  - defaults/osx-64::r-dplyr==0.8.0.1=r36h466af19_0\n",
      "  - defaults/noarch::r-cli==1.1.0=r36h6115d3f_0\n",
      "  - defaults/osx-64::r-tibble==2.1.1=r36h46e59ec_0\n",
      "  - defaults/osx-64::r-purrr==0.3.2=r36h46e59ec_0\n",
      "  - defaults/osx-64::r-sys==3.2=r36h46e59ec_0\n",
      "  - defaults/osx-64::r-rlang==0.3.4=r36h46e59ec_0\n",
      "  - defaults/noarch::r-knitr==1.22=r36h6115d3f_0\n",
      "  - defaults/osx-64::cffi==1.14.5=py37h2125817_0\n",
      "  - defaults/osx-64::anaconda-navigator==1.9.12=py37_0\n",
      "  - defaults/noarch::r-config==0.3=r36h6115d3f_0\n",
      "  - defaults/osx-64::r-mime==0.6=r36h46e59ec_0\n",
      "  - defaults/noarch::r-rprojroot==1.3_2=r36h6115d3f_0\n",
      "  - defaults/noarch::r-withr==2.1.2=r36h6115d3f_0\n",
      "  - defaults/osx-64::r-rcpp==1.0.1=r36h466af19_0\n",
      "  - defaults/osx-64::r-markdown==0.9=r36h46e59ec_0\n",
      "  - defaults/noarch::r-forcats==0.4.0=r36h6115d3f_0\n",
      "  - defaults/osx-64::cryptography==2.9.2=py37ha12b0ac_0\n",
      "  - defaults/osx-64::r-pki==0.1_5.1=r36h46e59ec_1\n",
      "  - defaults/noarch::r-sparklyr==1.0.0=r36h6115d3f_0\n",
      "  - defaults/osx-64::r-backports==1.1.4=r36h46e59ec_0\n",
      "  - defaults/noarch::r-cellranger==1.1.0=r36h6115d3f_0\n",
      "  - defaults/osx-64::r-htmltools==0.3.6=r36h466af19_0\n",
      "  - defaults/noarch::r-crayon==1.3.4=r36h6115d3f_0\n",
      "  - defaults/noarch::r-blob==1.1.1=r36h6115d3f_0\n",
      "  - defaults/osx-64::harfbuzz==2.4.0=h831d699_1\n",
      "  - defaults/noarch::pyopenssl==19.1.0=py_1\n",
      "  - defaults/noarch::requests==2.26.0=pyhd3eb1b0_0\n",
      "  - defaults/noarch::r-assertthat==0.2.1=r36h6115d3f_0\n",
      "  - defaults/noarch::jupyterlab_server==1.2.0=py_0\n",
      "  - defaults/noarch::r-prettyunits==1.0.2=r36h6115d3f_0\n",
      "  - defaults/noarch::r-rsconnect==0.8.13=r36h6115d3f_0\n",
      "  - defaults/osx-64::r-later==0.8.0=r36h466af19_0\n",
      "  - defaults/noarch::r-pkgconfig==2.0.2=r36h6115d3f_0\n",
      "  - defaults/osx-64::r-tidyr==0.8.3=r36h466af19_0\n",
      "  - defaults/osx-64::r-ellipsis==0.1.0=r36h46e59ec_0\n",
      "  - defaults/osx-64::r-rappdirs==0.3.1=r36h46e59ec_0\n",
      "  - defaults/osx-64::_anaconda_depends==2020.07=py37_0\n",
      "  - defaults/noarch::r-plogr==0.2.0=r36h6115d3f_0\n",
      "  - defaults/noarch::r-progress==1.2.0=r36h6115d3f_0\n",
      "  - defaults/noarch::r-rstudioapi==0.10=r36h6115d3f_0\n",
      "  - defaults/noarch::r-r2d3==0.2.3=r36h6115d3f_0\n",
      "  - defaults/osx-64::gevent==20.6.2=py37haf1e3a3_0\n",
      "  - defaults/osx-64::r-curl==3.3=r36h46e59ec_0\n",
      "  - defaults/osx-64::anaconda-client==1.7.2=py37_0\n",
      "  - defaults/noarch::r-r6==2.4.0=r36h6115d3f_0\n",
      "  - defaults/noarch::r-clipr==0.6.0=r36h6115d3f_0\n",
      "  - defaults/osx-64::r-jsonlite==1.6=r36h46e59ec_0\n",
      "  - defaults/osx-64::r-readr==1.3.1=r36h466af19_0\n",
      "  - defaults/osx-64::conda==4.10.3=py37hecd8cb5_0\n",
      "  - defaults/osx-64::pango==1.45.3=h7a6c989_0\n",
      "  - defaults/noarch::r-httr==1.4.0=r36h6115d3f_0\n",
      "  - defaults/noarch::r-rjdbc==0.2_7.1=r36h6115d3f_0\n",
      "  - defaults/osx-64::spyder==4.1.4=py37_0\n",
      "  - defaults/noarch::r-pillar==1.3.1=r36h6115d3f_0\n",
      "  - defaults/noarch::sphinx==3.2.1=py_0\n",
      "  - defaults/noarch::jupyterlab==2.1.5=py_0\n",
      "  - defaults/noarch::r-magrittr==1.5=r36h6115d3f_4\n",
      "  - defaults/osx-64::_ipyw_jlab_nb_ext_conf==0.1.0=py37_0\n",
      "  - defaults/noarch::r-htmlwidgets==1.3=r36h6115d3f_0\n",
      "  - defaults/noarch::r-highr==0.8=r36h6115d3f_0\n",
      "  - defaults/osx-64::r-readxl==1.3.1=r36h466af19_0\n",
      "  - defaults/noarch::r-odbc==1.1.5=r36h0a44026_0\n",
      "  - defaults/osx-64::r-rcurl==1.95_4.12=r36h46e59ec_0\n",
      "  - defaults/osx-64::r-utf8==1.1.4=r36h46e59ec_0\n",
      "  - defaults/osx-64::r-httpuv==1.5.1=r36h466af19_0\n",
      "  - defaults/noarch::r-xtable==1.8_4=r36h6115d3f_0\n",
      "  - defaults/noarch::r-hms==0.4.2=r36h6115d3f_0\n",
      "  - defaults/noarch::r-miniui==0.1.1.1=r36h6115d3f_0\n",
      "  - defaults/noarch::r-evaluate==0.13=r36h6115d3f_0\n",
      "  - defaults/osx-64::r-glue==1.3.1=r36h46e59ec_0\n",
      "  - defaults/osx-64::r-bit==1.1_14=r36h46e59ec_0\n",
      "  - defaults/osx-64::r-rjsonio==1.3_1.1=r36h466af19_0\n",
      "  - defaults/noarch::r-rematch==1.0.1=r36h6115d3f_0\n",
      "  - conda-forge/noarch::flask-bcrypt==0.7.1=py_1\n",
      "  - defaults/osx-64::anaconda==custom=py37_1\n",
      "  - defaults/osx-64::conda-build==3.18.9=py37_3\n",
      "  - defaults/noarch::r-dbi==1.0.0=r36h6115d3f_0\n",
      "  - defaults/osx-64::r-sourcetools==0.1.7=r36h466af19_0\n",
      "  - defaults/osx-64::clang_osx-64==10.0.0=h05bbb7f_0\n",
      "  - defaults/osx-64::brotlipy==0.7.0=py37haf1e3a3_1000\n",
      "done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Applications/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - beautifulsoup4\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    glib-2.69.1                |       hdf23fa2_0         2.7 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         2.7 MB\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  glib                                    2.63.1-hd977a24_0 --> 2.69.1-hdf23fa2_0\n",
      "  libffi                                3.2.1-h0a44026_1007 --> 3.3-hb1e8313_2\n",
      "\n",
      "The following packages will be DOWNGRADED:\n",
      "\n",
      "  llvm-openmp                             12.0.0-h0dcd299_1 --> 10.0.0-h28b9765_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "glib-2.69.1          | 2.7 MB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install --yes beautifulsoup4\n",
    "!conda install --yes lxml\n",
    "!conda install --yes requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's import the libraries we will be using in this Jupyter Notebook, including Beautiful Soup 4. The other two, requests and pandas, are already installed if you are using Anaconda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import lxml\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to make an HTTP request using the requests library. This means that once you have established a connection with the destination (the server which hosts the website you want to communicate with), the client (you) sends an HTTP GET request to the server to retrieve the website and all data within it. \n",
    "\n",
    "This is typically done by your web browser, but we can also do it in python. \n",
    "\n",
    "Today we are going to scrape information about players on the UVA men's basketball team. You can find the team's roster for the 2021-2022 season here: https://virginiasports.com/sports/mbball/roster/ \n",
    "\n",
    "We will start by gathering information about one of UVA's players, Kihei Clark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "#This is how to make an HTTP GET request using the requests library.\n",
    "source = requests.get('https://virginiasports.com/sports/mbball/roster/season/2021-22/player/kihei-clark/')\n",
    "\n",
    "print(source)      #this prints the type of response. 200 means \"OK\". There are many response codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a response from the server. If the response is good, the source code of the web page is contained within that response. Let's see what that looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(source.text, 'lxml')\n",
    "\n",
    "print(soup) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is messy, but it is all the code for the page we have issued a request for. Some of it is human readable, some of it is not. Now, let's look at the source code of this page another way. Copy and paste this link into your web browser: https://virginiasports.com/sports/mbball/roster/season/2021-22/player/kihei-clark/\n",
    "\n",
    "### Note - You need to use Google Chrome to have access to inspector and other developer tools\n",
    "\n",
    "Right click somewhere on your page and click \"inspect\". Then, make sure to choose the 'elements' tab to see the HTML source code of this page. While inspecting the page elements, you can see which parts of the page are controlled by different parts of the code. Notice that the code starts with large chunks (< body > for example), and has divisions within that (< div > tags), among others.\n",
    "\n",
    "The class \"bio-info\" looks like it contains the majority of the information in the body of the page. Let's start with this. We are going to scrape some information about Kihei Clark from this page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the prettify() function makes the code somewhat more readable.\n",
    "# I don't use this feature much but maybe you will appreciate it.\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The find() function finds the first item matching this criteria. Notice our arguments are first, the HTML tag, and second, the class within that tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"bio-info\">\n",
      "<div class=\"text-block\">\n",
      "<h1>Kihei Clark</h1>\n",
      "<div class=\"info-block\">\n",
      "<div>\n",
      "<div class=\"value\">Guard</div>\n",
      "<div class=\"description\">Position</div>\n",
      "</div>\n",
      "<div>\n",
      "<div class=\"value\">5'10''</div>\n",
      "<div class=\"description\">Height</div>\n",
      "</div>\n",
      "<div>\n",
      "<div class=\"value\">172 lbs.</div>\n",
      "<div class=\"description\">Weight</div>\n",
      "</div>\n",
      "<div>\n",
      "<div class=\"value\">Senior</div>\n",
      "<div class=\"description\">Class</div>\n",
      "</div>\n",
      "<div>\n",
      "<div class=\"value\">Woodland Hills, Calif.</div>\n",
      "<div class=\"description\">Hometown</div>\n",
      "</div>\n",
      "<div>\n",
      "<div class=\"value\">Taft Charter</div>\n",
      "<div class=\"description\">High School / Club</div>\n",
      "</div>\n",
      "<div>\n",
      "<div class=\"value\"><a href=\"https://twitter.com/ClarkKihei\" rel=\"nofollow\" target=\"_blank\"><i class=\"fab fa-twitter\"></i> @ClarkKihei</a></div>\n",
      "<div class=\"description\">Twitter</div>\n",
      "</div>\n",
      "<div>\n",
      "<div class=\"value\"><a href=\"https://instagram.com/kihei.clark/\" rel=\"nofollow\" target=\"_blank\"><i class=\"fab fa-instagram\"></i> @kihei.clark</a></div>\n",
      "<div class=\"description\">Instagram</div>\n",
      "</div>\n",
      "</div>\n",
      "</div>\n",
      "<div class=\"bio-image\" style=\"background-image: url(https://virginiasports.com/imgproxy/SxUO-Efz_hzvk_rIB9lpZJE2JwqfqYTUdE1j2nEI1vs/fit/600/800/ce/0/aHR0cHM6Ly9zdG9yYWdlLmdvb2dsZWFwaXMuY29tL3Zpcmdpbmlhc3BvcnRzLWNvbS8yMDIwLzExLzk0NWI2YmZmLWNsYXJrX2tpaGVpX3dlYi5qcGc.jpg)\" title=\"Kihei Clark - Men's Basketball - Virginia Cavaliers\"></div>\n",
      "<img alt=\"Kihei Clark - Men's Basketball - Virginia Cavaliers\" class=\"sr-only\" src=\"https://virginiasports.com/imgproxy/SxUO-Efz_hzvk_rIB9lpZJE2JwqfqYTUdE1j2nEI1vs/fit/600/800/ce/0/aHR0cHM6Ly9zdG9yYWdlLmdvb2dsZWFwaXMuY29tL3Zpcmdpbmlhc3BvcnRzLWNvbS8yMDIwLzExLzk0NWI2YmZmLWNsYXJrX2tpaGVpX3dlYi5qcGc.jpg\" title=\"Kihei Clark - Men's Basketball - Virginia Cavaliers\"/>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "player_info = soup.find(\"div\", class_='bio-info')    #class_, because 'class' is a reserved word in python\n",
    "print(player_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now see that the player's name is inside a couple more tags inside that 'bio-info' class. Let's drill down into the code and get the player's name value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kihei Clark\n"
     ]
    }
   ],
   "source": [
    "player_name = soup.find('div', class_='bio-info').div.h1.text\n",
    "print(player_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I want more information about the player such as his position, height, weight, etc. \n",
    "\n",
    "Looking again at the HTML, it looks like all of that information is in repetitive 'div' tags. Let's select all of those elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Guard', \"5'10''\", '172 lbs.', 'Senior', 'Woodland Hills, Calif.', 'Taft Charter', ' @ClarkKihei', ' @kihei.clark']\n"
     ]
    }
   ],
   "source": [
    "for item in soup.find_all('div', class_=\"value\"):\n",
    "    print(item.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also want to get the labels for this data. This would be things such as 'position', 'height', 'weight', etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position\n",
      "Height\n",
      "Weight\n",
      "Class\n",
      "Hometown\n",
      "High School / Club\n",
      "Twitter\n",
      "Instagram\n"
     ]
    }
   ],
   "source": [
    "for item in soup.find_all('div', class_='description'):\n",
    "    print(item.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do a little more here just to clean things up. In the following cell I will do the same as the previous two cells, but this time I will store the data in a list. Then I will zip those lists together and make a nice dictionary with my data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Position': 'Guard', 'Height': \"5'10''\", 'Weight': '172 lbs.', 'Class': 'Senior', 'Hometown': 'Woodland Hills, Calif.'}\n"
     ]
    }
   ],
   "source": [
    "player_stats=[]\n",
    "labels=[]\n",
    "\n",
    "for i in soup.find_all('div', class_='value'):\n",
    "    player_stats.append(i.text)\n",
    "    \n",
    "for i in soup.find_all('div', class_='description'):\n",
    "    labels.append(i.text)\n",
    "    \n",
    "#let's assume I only want the first five items in each list\n",
    "player_stats = player_stats[:5]\n",
    "labels = labels[:5]\n",
    "\n",
    "player_dict = dict(zip(labels, player_stats))\n",
    "print(player_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have an easy to use dictionary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172 lbs.\n"
     ]
    }
   ],
   "source": [
    "print(player_dict['Weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium\n",
    "\n",
    "Now that we have learned to scrape static HTML content, let's automate this task using Selenium. Check out the Selenium documentation here: https://www.selenium.dev/\n",
    "\n",
    "### Selenium Web Driver\n",
    "In order to use Selenium, we must download and install a web driver which allows you to drive a browser with your code.\n",
    "\n",
    "**Important**\n",
    "The following code assumes you are using Google Chrome and will use the associated web driver. If you are using another browser (safari, firefox, edge, etc) you will need to download the selenium web driver for that browser. Just check the Selenium documentation in order to do that.\n",
    "\n",
    "You also need to make sure to download the correct web driver which corresponds to your version of Google Chrome.\n",
    "\n",
    "I have included a detailed writeup about this in the 'WebDriverInstall.md' file in the github repository.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# install selenium\n",
    "!conda install --yes selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import selenium and webdrivers\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure now that Selenium is working and you have all your paths set up correctly. **Important**: You will have to change the path below to the path of 'driver' on your own computer.\n",
    "\n",
    "If this works correctly, it will open up a blank browser with a message that 'this is being controlled by automated test software'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that Selenium is working and you have all your paths set up correctly\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/ep9k/Desktop/PythonWebScraping-master/chromedriver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go directly to a page of our choice like so..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(executable_path=\"/Users/ep9k/Desktop/PythonWebScraping-master/chromedriver\")\n",
    "driver.get('https://virginiasports.com/sports/mbball/roster/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now proceed to write out script just like any other program. Just like BeautifulSoup, Selenium provides the ability to select HTML elements by the tag name, class name, id name, and so on. \n",
    "\n",
    "On the UVA roster homepage, looking at the HTML you can see that each player has it's own box with a picture, name, and so on. The HTML is the same for all of those players. In our example, we will simulate that the user is clicking on each player to see that player's individual page. Then we will scrape the information off of that page just like we did in earler. Let's start with just one player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(executable_path=\"/Users/ep9k/Desktop/PythonWebScraping-master/chromedriver\")\n",
    "driver.get('https://virginiasports.com/sports/mbball/roster/')\n",
    "\n",
    "#I am selecting by x path.\n",
    "player = driver.find_element_by_xpath('//*[@id=\"players\"]/div[1]/div[2]/div[1]')\n",
    "player.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See above that I used the 'find_element_by_xpath' method to click on this player's name. Let's define what an XPath is.\n",
    "\n",
    "**XPath**: XPath enables testers to navigate through the XML structure of an HTML or XML document. Don't worry too much about this. I like to use 'find_element_by_xpath' because it is basically a unique identifier for items in the HTML document and makes selecting them easy.\n",
    "\n",
    "There are many different ways to select HTML elements in Selenium. Check out the documentation for more examples: https://selenium-python.readthedocs.io/locating-elements.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have Selenium installed and set up and we did a small example, we can now expand upon it. We will take just a few players from the team and iterate through them. Once we land on each page we will do exactly what we just did with static HTML with BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kihei Clark\n",
      "Guard\n",
      "5'10''\n",
      "172 lbs.\n",
      "Senior\n",
      "Woodland Hills, Calif.\n",
      "Taft Charter\n",
      " @ClarkKihei\n",
      " @kihei.clark\n",
      "\n",
      "Jayden Gardner\n",
      "Forward\n",
      "6'6''\n",
      "246 lbs.\n",
      "Senior\n",
      "Wake Forest, N.C.\n",
      "Heritage\n",
      "East Carolina\n",
      " @Jayy_Baller_1\n",
      " @ssjjaygee\n",
      "\n",
      "Reece  Beekman\n",
      "Guard\n",
      "6'2''\n",
      "181 lbs.\n",
      "Sophomore\n",
      "Baton Rouge, La.\n",
      "Scotlandville Magnet\n",
      " @reece_beekman\n",
      " @reece.2\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(executable_path=\"/Users/ep9k/Desktop/PythonWebScraping-master/chromedriver\")\n",
    "driver.get('https://virginiasports.com/sports/mbball/roster/')\n",
    "\n",
    "\n",
    "#we will use a while loop to grab data about the first three players\n",
    "count = 0\n",
    "\n",
    "while count < 3:\n",
    "    count += 1\n",
    "    time.sleep(1)\n",
    "    \n",
    "    #select player's name using find_element_by_xpath()\n",
    "    #each player's name is a link to their bio page\n",
    "    player = driver.find_element_by_xpath(f'//*[@id=\"players\"]/div[{count}]/div[2]/div[1]')\n",
    "    player.click()\n",
    "    \n",
    "    #now we use beautiful soup to parse the HTML just as we did last time\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    \n",
    "    player_name = soup.find('div', class_='bio-info').div.h1.text\n",
    "    print()\n",
    "    print(player_name)\n",
    "    \n",
    "    for item in soup.find_all('div', class_=\"value\"):\n",
    "        print(item.text) \n",
    "        \n",
    "    #go back to roster page after collecting information about current player    \n",
    "    driver.get('https://virginiasports.com/sports/mbball/roster/')\n",
    "    \n",
    "driver.quit()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do just a little more to make it pretty. This time we will collect the information about each player and put it into a pandas dataframe. There are many ways to do this but I will use lists to populate the columns of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Name, Position, Height, Weight, Year]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['Name', 'Position', 'Height', 'Weight', 'Year'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the whole thing together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/ep9k/Desktop/PythonWebScraping-master/chromedriver\")\n",
    "driver.get('https://virginiasports.com/sports/mbball/roster/')\n",
    "\n",
    "#accumulator lists we will use later to make our pandas dataframe\n",
    "names = []\n",
    "positions = []\n",
    "heights = []\n",
    "weights = []\n",
    "years = []\n",
    "\n",
    "\n",
    "#we will use a while loop to grab data about the first three players\n",
    "count = 0\n",
    "\n",
    "team_players = ['Kihei Clark', 'Jayden Gardner', 'Reece Beekman', 'Armaan Franklin',\n",
    "               'Jayden Nixon', 'Taine Murray', 'Malachi Poindexter', 'Chase Coleman',\n",
    "               'Kadin Shedrick', 'Francisco Caffaro', 'Kody Stattman', 'Igor Milicic Jr',\n",
    "               'Carson McCorkle']\n",
    "\n",
    "#there are 13 players on the team\n",
    "for i in team_players:\n",
    "    count += 1\n",
    "    time.sleep(1)\n",
    "    \n",
    "    #select player's name using find_element_by_xpath()\n",
    "    #each player's name is a link to their bio page\n",
    "    player = driver.find_element_by_xpath(f'//*[@id=\"players\"]/div[{count}]/div[2]/div[1]')\n",
    "    player.click()\n",
    "    \n",
    "    #now we use beautiful soup to parse the HTML just as we did last time\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    \n",
    "    #player info will be used to store the data about each player in a list\n",
    "    player_info = []\n",
    "    \n",
    "    for item in soup.find_all('div', class_=\"value\"):\n",
    "        player_info.append(item.text)\n",
    "        \n",
    "    #add the information from player_info to the accumulator lists outside this loop\n",
    "    names.append(i)    # i is player name from team_players\n",
    "    positions.append(player_info[0])\n",
    "    heights.append(player_info[1])\n",
    "    weights.append(player_info[2])\n",
    "    years.append(player_info[3])\n",
    "        \n",
    "    #go back to roster page after collecting information about current player    \n",
    "    driver.get('https://virginiasports.com/sports/mbball/roster/')\n",
    "    \n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we will take the lists we created above and put them into the pandas dataframe we created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Name       Position  Height    Weight                Year\n",
      "0          Kihei Clark          Guard  5'10''  172 lbs.              Senior\n",
      "1       Jayden Gardner        Forward   6'6''  246 lbs.              Senior\n",
      "2        Reece Beekman          Guard   6'2''  181 lbs.           Sophomore\n",
      "3      Armaan Franklin          Guard   6'4''  204 lbs.              Junior\n",
      "4         Jayden Nixon          Guard   6'3''  200 lbs.              Senior\n",
      "5         Taine Murray          Guard   6'5''  207 lbs.            Freshman\n",
      "6   Malachi Poindexter          Guard   6'2''  190 lbs.           Sophomore\n",
      "7        Chase Coleman          Guard   5'9''  161 lbs.              Junior\n",
      "8       Kadin Shedrick        Forward  6'11''  231 lbs.  Redshirt Sophomore\n",
      "9    Francisco Caffaro         Center   7'1''  242 lbs.     Redshirt Junior\n",
      "10       Kody Stattman          Guard   6'8''  200 lbs.              Senior\n",
      "11     Igor Milicic Jr  Guard/Forward  6'10''  224 lbs.            Freshman\n",
      "12     Carson McCorkle          Guard   6'3''  185 lbs.           Sophomore\n"
     ]
    }
   ],
   "source": [
    "df['Name'] = names\n",
    "df['Position'] = positions\n",
    "df['Height'] = heights\n",
    "df['Weight'] = weights\n",
    "df['Year'] = years\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
