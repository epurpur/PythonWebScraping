{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Web Scraping in Python\n",
    "\n",
    "Today we will learn how to scrape HTML web pages in python, using the Beautiful Soup 4 library. We can programmatically gather information from websites to use for your own purposes. We will gather information about prominent UVA employees, first by walking through each step of the process. Then by doing it in a more automated way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to install the Beautiful Soup 4 and lxml libraries. These are not a part of base python or the Anaconda installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install --yes beautifulsoup4\n",
    "!conda install --yes lxml\n",
    "!conda install --yes requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's import the libraries we will be using in this Jupyter Notebook, including Beautiful Soup 4. The other two, requests and pandas, are already installed if you are using Anaconda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to make an HTTP request using the requests library. This means that once you have established a connection with the destination (the server which hosts the website you want to communicate with), the client (you) sends an HTTP GET request to the server to retrieve the website and all data within it. \n",
    "\n",
    "This is typically done by your web browser, but we can also do it in python. \n",
    "\n",
    "Today we are going to scrape information about players on the UVA men's basketball team. You can find the team's roster for the 2021-2022 season here: https://virginiasports.com/sports/mbball/roster/ \n",
    "\n",
    "We will start by gathering information about one of UVA's players, Kihei Clark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "#This is how to make an HTTP GET request using the requests library.\n",
    "source = requests.get('https://virginiasports.com/sports/mbball/roster/season/2021-22/player/kihei-clark/')\n",
    "\n",
    "print(source)      #this prints the type of response. 200 means \"OK\". There are many response codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a response from the server. If the response is good, the source code of the web page is contained within that response. Let's see what that looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(source.text, 'html.parser')\n",
    "\n",
    "print(soup) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is messy, but it is all the code for the page we have issued a request for. Some of it is human readable, some of it is not. Now, let's look at the source code of this page another way. Copy and paste this link into your web browser: https://virginiasports.com/sports/mbball/roster/season/2021-22/player/kihei-clark/\n",
    "\n",
    "### Note - You need to use Google Chrome to have access to inspector and other developer tools\n",
    "\n",
    "Right click somewhere on your page and click \"inspect\". Then, make sure to choose the 'elements' tab to see the HTML source code of this page. While inspecting the page elements, you can see which parts of the page are controlled by different parts of the code. Notice that the code starts with large chunks (< body > for example), and has divisions within that (< div > tags), among others.\n",
    "\n",
    "The class \"bio-info\" looks like it contains the majority of the information in the body of the page. Let's start with this. We are going to scrape some information about Kihei Clark from this page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the prettify() function makes the code somewhat more readable.\n",
    "# I don't use this feature much but maybe you will appreciate it.\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The find() function finds the first item matching this criteria. Notice our arguments are first, the HTML tag, and second, the class within that tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"bio-info\">\n",
      "<div class=\"text-block\">\n",
      "<h1>Kihei Clark</h1>\n",
      "<div class=\"info-block\">\n",
      "<div>\n",
      "<div class=\"value\">Guard</div>\n",
      "<div class=\"description\">Position</div>\n",
      "</div>\n",
      "<div>\n",
      "<div class=\"value\">5'10''</div>\n",
      "<div class=\"description\">Height</div>\n",
      "</div>\n",
      "<div>\n",
      "<div class=\"value\">172 lbs.</div>\n",
      "<div class=\"description\">Weight</div>\n",
      "</div>\n",
      "<div>\n",
      "<div class=\"value\">Senior</div>\n",
      "<div class=\"description\">Class</div>\n",
      "</div>\n",
      "<div>\n",
      "<div class=\"value\">Woodland Hills, Calif.</div>\n",
      "<div class=\"description\">Hometown</div>\n",
      "</div>\n",
      "<div>\n",
      "<div class=\"value\">Taft Charter</div>\n",
      "<div class=\"description\">High School / Club</div>\n",
      "</div>\n",
      "<div>\n",
      "<div class=\"value\"><a href=\"https://twitter.com/ClarkKihei\" rel=\"nofollow\" target=\"_blank\"><i class=\"fab fa-twitter\"></i> @ClarkKihei</a></div>\n",
      "<div class=\"description\">Twitter</div>\n",
      "</div>\n",
      "<div>\n",
      "<div class=\"value\"><a href=\"https://instagram.com/kihei.clark/\" rel=\"nofollow\" target=\"_blank\"><i class=\"fab fa-instagram\"></i> @kihei.clark</a></div>\n",
      "<div class=\"description\">Instagram</div>\n",
      "</div>\n",
      "</div>\n",
      "</div>\n",
      "<div class=\"bio-image\" style=\"background-image: url(https://virginiasports.com/imgproxy/SxUO-Efz_hzvk_rIB9lpZJE2JwqfqYTUdE1j2nEI1vs/fit/600/800/ce/0/aHR0cHM6Ly9zdG9yYWdlLmdvb2dsZWFwaXMuY29tL3Zpcmdpbmlhc3BvcnRzLWNvbS8yMDIwLzExLzk0NWI2YmZmLWNsYXJrX2tpaGVpX3dlYi5qcGc.jpg)\" title=\"Kihei Clark - Men's Basketball - Virginia Cavaliers\"></div>\n",
      "<img alt=\"Kihei Clark - Men's Basketball - Virginia Cavaliers\" class=\"sr-only\" src=\"https://virginiasports.com/imgproxy/SxUO-Efz_hzvk_rIB9lpZJE2JwqfqYTUdE1j2nEI1vs/fit/600/800/ce/0/aHR0cHM6Ly9zdG9yYWdlLmdvb2dsZWFwaXMuY29tL3Zpcmdpbmlhc3BvcnRzLWNvbS8yMDIwLzExLzk0NWI2YmZmLWNsYXJrX2tpaGVpX3dlYi5qcGc.jpg\" title=\"Kihei Clark - Men's Basketball - Virginia Cavaliers\"/>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "player_name = soup.find(\"div\", class_='bio-info')    #class_, because 'class' is a reserved word in python\n",
    "print(player_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now see that the player's name is inside a couple more tags inside that 'bio-info' class. Let's drill down into the code and get the player's name value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kihei Clark\n"
     ]
    }
   ],
   "source": [
    "player_name = soup.find('div', class_='bio-info').div.h1.text\n",
    "print(player_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I want more information about the player such as his position, height, weight, etc. \n",
    "\n",
    "Looking again at the HTML, it looks like all of that information is in repetitive 'div' tags. Let's select all of those elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guard\n",
      "5'10''\n",
      "172 lbs.\n",
      "Senior\n",
      "Woodland Hills, Calif.\n",
      "Taft Charter\n",
      " @ClarkKihei\n",
      " @kihei.clark\n"
     ]
    }
   ],
   "source": [
    "for item in soup.find_all('div', class_=\"value\"):\n",
    "    print(item.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also want to get the labels for this data. This would be things such as 'position', 'height', 'weight', etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position\n",
      "Height\n",
      "Weight\n",
      "Class\n",
      "Hometown\n",
      "High School / Club\n",
      "Twitter\n",
      "Instagram\n"
     ]
    }
   ],
   "source": [
    "for item in soup.find_all('div', class_='description'):\n",
    "    print(item.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do a little more here just to clean things up. In the following cell I will do the same as the previous two cells, but this time I will store the data in a list. Then I will zip those lists together and make a nice dictionary with my data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Position': 'Guard', 'Height': \"5'10''\", 'Weight': '172 lbs.', 'Class': 'Senior', 'Hometown': 'Woodland Hills, Calif.'}\n"
     ]
    }
   ],
   "source": [
    "player_stats=[]\n",
    "labels=[]\n",
    "\n",
    "for i in soup.find_all('div', class_='value'):\n",
    "    player_stats.append(i.text)\n",
    "    \n",
    "for i in soup.find_all('div', class_='description'):\n",
    "    labels.append(i.text)\n",
    "    \n",
    "#let's assume I only want the first five items in each list\n",
    "player_stats = player_stats[:5]\n",
    "labels = labels[:5]\n",
    "\n",
    "player_dict = dict(zip(labels, player_stats))\n",
    "print(player_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have an easy to use dictionary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172 lbs.\n"
     ]
    }
   ],
   "source": [
    "print(player_dict['Weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start here\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A common thing you might do is take this data and use it for your own purposes elsewhere.\n",
    "#Let's take this data we have scraped and put it into a pandas DataFrame\n",
    "\n",
    "data = []\n",
    "\n",
    "for name in formatted_names_of_important_people:\n",
    "    \n",
    "    source = requests.get(f'https://data.richmond.com/salaries/2018/state/university-of-virginia/{name}')\n",
    "    soup = BeautifulSoup(source.text, 'html.parser')\n",
    "\n",
    "    main_box = soup.find(\"div\", class_='pay')\n",
    "    salary = main_box.find('h2').text\n",
    "    \n",
    "    #data.append((name, salary))\n",
    "    data.append((name, salary))\n",
    "    \n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put this into a pandas Dataframe!\n",
    "\n",
    "## Pandas ##\n",
    "\n",
    "Pandas is an open source python library providing high-performance, easy to use data structures. It is common to store data scraped from the web in a pandas Dataframe.\n",
    "\n",
    "A pandas Dataframe is a 2 Dimensional data structure with rows and columns (like a spreadsheet). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's rename the columns in the pandas dataframe to something more descriptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.columns = ['Name', 'Salary']\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
